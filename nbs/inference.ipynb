{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, os.path.join(os.getcwd(),\"../src/\"))# this should vary when using Colab\n",
    "\n",
    "from dcgan import DCGAN\n",
    "from config import *\n",
    "from dataset import WaterBodyGeneratorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS TO INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_cfg.models = inference_cfg.models   # [\"model name\"]\n",
    "inference_cfg.configs = inference_cfg.configs # [ config_name]\n",
    "inference_cfg.epoch = inference_cfg.epoch     # [epoch]\n",
    "inference_cfg.gt = inference_cfg.gt           # True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, 100,1,1, device=\"cuda\")\n",
    "content = []\n",
    "\n",
    "# CREATE MODELS\n",
    "models = [ DCGAN(config) for config in inference_cfg.configs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE INFERENCE BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    for epoch in inference_cfg.epoch:\n",
    "        model.load(os.path.join(inference_cfg.root_path, inference_cfg.models[i]), epoch)\n",
    "        model.eval()\n",
    "        batch = model.generate_batch(batch_size=inference_cfg.batch_size, threeshold=inference_cfg.threeshold,batch_gen_size=inference_cfg.batch_gen_size)\n",
    "        batch = [(x[0],np.transpose(x[1],(1,2,0))) for x in batch]\n",
    "        content.append(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE GT FOR COMPARISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE GROUND TRUTH\n",
    "if inference_cfg.gt:\n",
    "    tf = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(64),\n",
    "            transforms.CenterCrop(64),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean    = [0.5, 0.5, 0.5],\n",
    "                std     = [0.5, 0.5, 0.5])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    path = \"D:/AI_ML/Kaggle/Water Bodies Dataset_pruned_more/\"\n",
    "    dataset = WaterBodyGeneratorDataset(path, transform=tf)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "    gt = [(\"gt\",np.transpose(o,(1,2,0))) for o in iter(dataloader).next()[:inference_cfg.batch_size]]\n",
    "    content.append(gt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISPLAY IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(content),inference_cfg.batch_size, figsize=(20,20))\n",
    "counter = 0\n",
    "e = 0\n",
    "m=0\n",
    "for i in range(len(content)):\n",
    "    if e >= len(inference_cfg.epoch):\n",
    "        e = 0\n",
    "        m+=1\n",
    "    for j in range(inference_cfg.batch_size):\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        white = content[i][counter][1].max()\n",
    "        black = content[i][counter][1].min()\n",
    "        arr = (content[i][counter][1] - black)* (1/(white-black))\n",
    "        if inference_cfg.gt and i== len(content) -1:\n",
    "            axs[i][0].set_ylabel(\"ground truth\")\n",
    "        else:\n",
    "\n",
    "            axs[i][0].set_ylabel(f'{inference_cfg.models[m]}_{inference_cfg.epoch[e]}')\n",
    "\n",
    "        axs[i][j].set_title(str(content[i][counter][0]))\n",
    "        #print(arr)\n",
    "        axs[i][j].imshow(arr)\n",
    "        counter +=1\n",
    "    e +=1\n",
    "    counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81c5ca5e4a0af33c0e7274a0de2a298af99413d99e7702ad60c9f85312d1d29c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
